---
title: ''
author: ''
csl: aje_style.csl
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage[T1]{fontenc}
- \usepackage{microtype}
- \usepackage[margin=1in]{geometry}
- \usepackage{fancyhdr}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead{}
- \fancyfoot{}
- \fancyhead[C]{Multiple outcomes}
- \fancyfoot[RO,LE]{\thepage}
- \usepackage{booktabs}
- \usepackage{lettrine}
- \usepackage{paralist}
- \usepackage{setspace}\singlespacing
- \usepackage{url}
- \usepackage{parskip}
- \usepackage{color,soul}
- \usepackage{palatino}
- \usepackage{float}
bibliography: refs_fake.bib
---

\doublespacing

\begin{center}
\textbf{ \LARGE{Project Log: Multiple Outcomes} }
\vspace{10mm}
\end{center}

\doublespacing



\tableofcontents

\section{Stream of consciousness}



Mystery #2: Why does Westfall recommend a resampling scheme for logistic regression that doesn't preserve correlation? (pg 215)

Multiple imputation idea: Maybe a general algorithm for resampling under $H_A$ could be something like MI. You would take your dataset and copy all the Xs, deleting the Ys. Then you would use an off-the-rack parametric MI procedure to impute the "missing data" for each of the resamples (and each time delete the observed data). E.g., could do a fully conditional approach where you first regression Y1 on all other variables, including the other outcomes, then regress Y2 on the remaining ones, etc. This does invoke assumptions about the form of relationship between the outcomes, but at least wouldn't break any parametric assumptions. I think you could do this with MICE's off-the-rack models because it fits logistic regression to binary variables by default (see JStatSoft paper)! To resample under $H_0$ this way, could first shuffle the Ys in original dataset? That should preserve correlation between Ys. 


\section{For Westfall simulations}

* Redo as-is but with 1000 reps/scenario, and have more reps in doParallel
* Add Holm to simulations -- maybe do it along with Romano?
* Add Romano to simulations (requires FCR instead of Westfall approach) since it seems to work in practice (\texttt{debug\_fcr}), but remember that this violates OLS assumptions, so isn't technically advisable. 

\section{For later simulations}

* Add binary outcomes with logistic regression?
* To further improve power, could Use something about p-values themselves for joint test?




\section{FWER control methods we should compare for power of joint test}

\begin{enumerate}
\item Westfall's single-step minP and maxT methods (pg 47): Easy to code manually
\item Westfall's step-down free method (pg 66): Moderate to code manually
\item Romano: Implemented in StepwiseTest package, but needs debugging
\item Holm (step-down): Easy to code manually; UMP than Bonferroni
\item Bonferroni (single-step): Easy to code manually
\end{enumerate}


\section{Our two metrics}

\textbf{Metric \#1}: 95\% CI for number of hits above expected value that we see in observed data (resample under original distribution, using first half of my theory)

\textbf{Metric \#2}: 95\% CI under the null for number of hits (resample under null, using second half of my theory)

What about other link functions or hypothesis tests not based on regression? How would we do residual resampling? Davison \& Hinkley talk about other GLMs. I think we'd end up with either some form of residual or $Y$ itself for each outcome, and then we'd jointly resample these things as planned.



<!-- NEW SECTION -->
\section{Possible resampling algorithms}

\textbf{Conclusion:} Since we are bootstrapping and then fitting parametric model (OLS), I think we should use Fox's or Westfall's residual resampling approach (and indeed, this seemed to work in the last simulation). Full-case resampling violates the iid error assumption in OLS regression, meaning the probability of rejecting the null within each resample is not $\alpha$. Our application of bootstrapping is unusual in that we need to resample in a way that is compatible with parametric assumptions of the test we're going to apply for each outcome. 


Note that all of these methods assume $Var(Y^{*} | X^{*}=0) = Var(Y^{*} | X^{*}=1)$; in the resampled data, this is the average of the original within-stratum variances. I think this is fine since we are \emph{not} avoiding parametric inference for OLS (unlike in other bootstrapping applications). 

\underline{Basic "fix $X$, then resample $Y$"}
  \begin{enumerate}
  \item Fix $X$ and resample only $Y$: $(Y^{*}_i, X^{*}_i) = \left( \text{sample}(Y_i), X_i \right)$. So $Y^{*}_i \amalg X^{*}_i$. 
  \item Regress: $Y^{*}_i = \beta_0 + \beta X^{*}_i + \epsilon$.
  \item Test $H_0: \beta = 0$.
  \end{enumerate}
  
  
Has multimodality problem. No guarantee that residuals are normal, so OLS inference will be wrong anytime the original data are generated under $H_A$. 

\begin{figure}[H]
\centering
\caption{Original data.}
\includegraphics[width=100mm]{orig_data.png}
\end{figure}
  
\begin{figure}[H]
\centering
\caption{Resampled data under basic algorithm. Loses association between sex and Y, so regression residuals are bimodal.}
\includegraphics[width=100mm]{tyler.png}
\end{figure}

\underline{Westfall's "center, then resample $Y$"}
  \begin{enumerate}
  \item Fix $X$ and set $Y^{*}_i = \text{sample}\left( Y - \widehat{Y} \right)$. So $Y^{*}_i \amalg X^{*}_i$. 
  \item Regress: $Y^{*}_i = \beta_0 + \beta X^{*}_i + \epsilon$.
  \item Test $H_0: \beta = 0$.
  \end{enumerate}
  
  
Avoids multimodality problem, but could still have non-normal residual distribution, as in the above. So OLS inference could be wrong, but only if original data already violate homoskedasticity. Works well in my simulations. 

\begin{figure}[H]
\centering
\caption{Resampled data under Westfall's algorithm. Residuals are normal.}
\includegraphics[width=100mm]{westfall.png}
\end{figure}
  

\underline{Fox/Davison's residual resampling}

See Davison Algorithm 6.1 in "Linear Regression" chapter. 
\begin{enumerate}
  \item Fix $X$ and set $Y^{*}_i = \widehat{Y}_i + \text{sample}\left( Y - \widehat{Y} \right)$. So $Y^{*}_i \not\!\perp\!\!\!\perp X^{*}_i$.
  \item Regress: $Y^{*}_i = \beta_0 + \beta X_i + \epsilon$.
  \item Test $H_0: \beta = \widehat{\beta}$ (i.e., the fitted coefficient from original sample).
  \end{enumerate}
  
Avoids multimodality problem residuals will be normal as long as they are normal within strata of $X$, so OLS inference should be correct. $Var(Y^{*} | X^{*}=0) = Var(Y^{*} | X^{*}=0)$ and is an average of the original conditional variances. 
  
\begin{figure}[H]
\centering
\caption{Resampled data under Fox's algorithm. Residuals are normal.}
\includegraphics[width=100mm]{fox.png}
\end{figure}

<!-- NEW SECTION -->
\newpage
\section{Other online resources}

\subsection{$\star$ CV: Permutation tests vs. bootstrap tests}

What we are doing is similar to a permutation test because we're not centering the observations. In this answer, Snow says "permutation tests test a specific null hypothesis of exchangeability, i.e. that only the random sampling/randomization explains the difference seen". Also, permutation tests are more powerful. 

"Resample $Y$ while fixing $X$" strategy is "similar in spirit" to permutation tests and assumes that the distribution of $Y$ in each group of $X$ is identical. In contrast, "mean-center, then resample within each group of $X$" does not make this assumption. 



\url{https://stats.stackexchange.com/questions/20217/bootstrap-vs-permutation-hypotheis-testing}

\subsection{$\star$ CV: Center first or resample first?}

\url{https://stats.stackexchange.com/questions/136661/using-bootstrap-under-h0-to-perform-a-test-for-the-difference-of-two-means-repl/187630}

Goes over difference between "resample, then center" (OP's first bootstrap) and "center, then resample" (OP's second bootstrap). 

Accepted answer says that first approach is actually testing whether the distribution of x and y are identical. I DON'T REALLY GET THIS. 


\subsection{CV: Cases when naive bootstrap fails}

\url{https://stats.stackexchange.com/questions/9664/what-are-examples-where-a-naive-bootstrap-fails/9722#9722}

\subsection{CV: Applying bootstrap to arbitrary smooth function of the data}
\url{https://stats.stackexchange.com/questions/246632/smoothness-of-a-statistic-for-bootstrapping}




<!-- NEW SECTION -->
\newpage
\section{Notes on past literature}


\subsection{Other "global tests" (i.e., of joint null)}

\begin{itemize}
\item Fisher's Combination Test aggregates all the p-values instead of just using the smallest one; better than Bonferroni joint test if many small effects, but worse if there are a few large effects. Assumes independent tests. 
\end{itemize}


\subsection{Frane (per-family error rates)}

\begin{itemize}
\item Per-family error rate (PFER): Number of expected false positives 
\item Controlling PFER implies controlling FWER (so PFER is more stringent)
\item Bonferroni also controls PFER to $\alpha$, which is partly why it's more "conservative"
\end{itemize}

Talks about *another interesting interpretation of Bonferroni*, which is that (unlike other FWER-control procedures), it also controls the more stringent "per-family error rate" (PFER), which is the expected number of false positives. 

With other FWER procedures that don't control PFER, which is most of them, there is a <5\% probability of at least one false positive, but that could be 1 false positive or it could be 100. With Bonferroni, if there are false positives (<5\% chance), it's also the case that the expected number of them is $< 0.05 \times$ (# of tests) with <5\% probability.

So to build upon Tyler's earlier statement, I believe Bonferroni lets us say:

*"We did 100 tests and rejected 20. With >95\% confidence, all 20 are true effects (FWER). Even if we are in the 5% of instances where we actually do have false positives, there aren't very many of them: only 5 or fewer with 95\% confidence (PFER)."*


\subsection{Yekutieli \& Benjamini}

A parametric bootstrap correction for LMM.

\subsection{Yekutieli \& Benjamini}

A resampling-based procedure for FDR control. Very readable! 

\begin{itemize}
\item Pg. 7: Outlines residual resampling under $H_0$, exactly as we will be doing
\item Talks a lot about Westfall and also uses subset pivotality
\item Pg 7: "Subset pivotality is achieved if the distribution of [the p-values of the true null hypotheses] is unaffected by the truth or falsehood of the remaining hypotheses"
\item Seems to be very closely related to Westfall, but cares about FDR instead of FWER. 
\end{itemize}

\subsection{van der Laan, Dudoit, Pollard (2004) - not saved}

A resampling-based multiple testing control procedure. 

\begin{itemize}
\item $k-$FWER control procedure where you first reject hypotheses based on a procedure that controls regular FWER, but then also "augment" by rejecting an additional $k=1$ hypotheses (summarized by Romano \& Wolf, pg 1396)
\end{itemize}

\subsection{Romano \& Wolf}

A resampling-based multiple testing control procedure for FWER, k-FWER, or FDP. 

\begin{itemize}
\item $k-$FWER control: With 95\% confidence, the number of false positives is $< k$
\item FDP control: With 95\% confidence, the proportion of rejections that are false positives is $< \gamma$
\item FDR control: with 95\% confidence, the EXPECTED proportion of rejections that are false positives is $< \gamma$ 
\item They bootstrap under the original data distribution and use this to get quantiles of the distribution of the $k^{th}$-largest test statistic for various subsets of the test statistics, allowing control of the $k-$FWER through a step-down procedure. 
\item Why resample not under the null? Since we don't know which hypotheses are true, assume the data really were generated under the null because this is conservative. If the data aren't generated under the null, then the critical values will just be more stringent because the kth-largest test stat will be larger (page 1383). Basically this "exploits the duality between CIs and hypothesis tests" (pg 1388). Apparently resampling not under the null avoids subset pivotality (pg 1388). 
\item Page 1396: Talks about other resampling methods in some detail, including Westfall
\item Does not use subset pivotality, unlike Westfall
\end{itemize}

\subsection{Davison \& Hinkley text}

\begin{itemize}
\item Linear regression chapter has great explanation of case resampling (Tyler's approach) vs. residual resampling (Fox, sort of Westfall). 
\item Pg 12 in "Linear Regression" chapter: Says that with case resampling, "the null hypothesis being tested is stronger than just zero slope"
\end{itemize}

\subsection{Good (2005) book chapter}

\begin{itemize}
\item Clear explanation of difference in assumptions between nonparametric bootstrap (equal parameters) and permutation test (exchangeability). 
\item I think the "resample, then center" strategy is closer to a permutation test since it forces equal distributions, while the "center, then resample" or "just resample" strategies are more like bootstrapping. 
\end{itemize}

\subsection{Fox chapter on bootstrapping regressions}

\begin{itemize}
\item {In section on bootstrap hypothesis testing, uses this residual-resampling algorithm: 
  \begin{enumerate}
  \item Fix $X$ and set $Y^{*}_i = \widehat{Y}_i + \text{sample}\left( Y - \widehat{Y} \right)$. So $Y^{*}_i \not\!\perp\!\!\!\perp X^{*}_i$.
  \item Regress: $Y^{*}_i = \beta_0 + \beta X_i + \epsilon$.
  \item Test $H_0: \beta = \widehat{\beta}$ (i.e., the fitted coefficient from original sample).
  \end{enumerate}
}
\end{itemize}


\subsection{Hall \& Wilson "Two guidelines"}

\begin{itemize}
\item {"Sample, then center" algorithm (here, I am extrapolating what they present to the regression setting):
  \begin{enumerate}
  \item Resample entire vector $(Y^{*}_i, X^{*}_i) = \text{sample}\left( (Y_i, X_i) \right)$. So $Y^{*}_i \not\!\perp\!\!\!\perp X^{*}_i$. 
  \item Regress: $Y^{*}_i = \beta_0 + \beta X^{*}_i + \epsilon$.
  \item Test $H_0: \beta = \widehat{\beta}$ (i.e., the fitted coefficient from original sample).
  \end{enumerate}
}

\item The point of using asymptotically pivotal statistics (centered and scaled) is that then the asymptotic distribution of the statistic does not depend on any unknowns.

\item When you can’t estimate variance of estimator in order to create pivot, suggests using accelerated bias correction bootstrap.
\end{itemize}



\subsection{$\star$ Troendle 2004 "Slow convergence"}

\begin{itemize}

\item Talks about multiple 2-sample $t$-tests

\item{ Two approaches (see page 3 for very clear overview):
  \begin{enumerate}
  \item Permute a subject’s outcome variables while assigning group labels (with \emph{uncentered} variables)  
  \item Bootstrap by resampling a subject’s entire row (with \emph{centered} outcome variables)
  \end{enumerate}
}


\item {"Center, then sample" bootstrapping algorithm (here, I am extrapolating what they present to the regression setting):
  \begin{enumerate}
  \item Center outcome by setting $\widetilde{Y}_i = Y_i - \widehat{Y}$
  \item Resample entire vector: $(Y^{*}_i, X^{*}_i) = \text{sample}\left( (\widetilde{Y}_i, X_i) \right)$. So $Y^{*}_i \amalg X^{*}_i$. 
  \item Regress: $Y^{*}_i = \beta_0 + \beta X^{*}_i + \epsilon$.
  \item Test $H_0: \beta = 0$.
  \end{enumerate}
}

\end{itemize}


\subsection{Westfall \& Young textbook}

Resampling-based procedures for FWER control. 

\begin{itemize}

\item{"Center, then resample" strategy
  \begin{enumerate}
  \item Center outcome by setting $\widetilde{Y}_i = Y_i - \widehat{Y}$
  \item Fix $X$ and resample only $Y$: $(Y^{*}_i, X^{*}_i) = \left( \text{sample}(\widetilde{Y}_i), X_i \right)$. So $Y^{*}_i \amalg X^{*}_i$. 
  \item Regress: $Y^{*}_i = \beta_0 + \beta X^{*}_i + \epsilon$.
  \item Test $H_0: \beta = 0$.
  \end{enumerate}
}

\item \hl{Single-step adjusted p-value, aka "minP" method (page 46, 51): For each test $j$, set its adjusted p-value equal to the probability in the resamples (generated under the strong null) that the minimum p-value is less than the observed p-value for test $j$. So, it's like a p-value of the p-value (probability of observing a p-value at this small under the strong null)}

\item \hl{Free step-down resampling method (pg 66): Not very intuitive...}

\item Strong FWER control

\item 106-109: More on OLS

\item Page 79: "Resampling from the residuals, rather than the actual data values, is important. If the actual data are pooled and resampled, then the underlying residual distribution will be estimated using a multi-modal empirical distribution function when the means differ. Such resampling violates the first principle of Hall \& Wilson (1991): '...even if the data might be drawn in a way that fails to satisfy $H_0$, resampling should be done in a way that reflects $H_0$'." 

\item Page 39-41: Analyzes $P( P^* < \alpha )$ in bootstrap iterates

\item Page 92-93: Re-randomization

\item Bootstraps of pivotal statistics converge faster (Section 2.2.2)

\end{itemize}


\subsection{Westfall \& Troendle (2008)}

\begin{itemize}

\item \hl{Subset pivotality assumption:} Take any subset of the hypotheses to be tested. The distribution of the max test statistic within this group, given that all the nulls in that subset hold, is the same as its distribution given the strong null (all the nulls for ALL the hypothesis tests hold). In other words, once you know that all nulls within the subset are true, knowing that all the other nulls are true doesn't change anything about distribution of max test statistic in that subset. 

\item Reason for using subset pivotality: To avoid having to resample under every intersection null (i.e., subsets of nulls being true) so that we can just resample under the joint null. 

\item Setting: You have multiple outcomes measured on multiple categorical groups and are interested in full exchangeability on the outcome across the groups (not just different means, for example). 

\item Shows that "permutation methods are distribution-free under an appropriate exchangeability assumption and FWER follows mathematically, regardless of sample size"

\item Implemented in SAS’ PROC MULTTEST

\item According to SAS' documentation, this(?) paper shows that: "when subset pivotality holds, the joint distribution of p-values under the subset is identical to that under the complete null"

\end{itemize}


\subsection{Bretz, Hothorn, \& Westfall (2008)'s R paper}

\begin{itemize}

\item They are fixing the Y and permuting the Xs (in their application, these are just the group labels), so are keeping the correlation structure of the Ys as we are (pg. 130)

\item Bootstrapping vs. permutation pros and cons (pg. 140)"

\end{itemize}


\subsection{Bickel}

\begin{itemize}
\item Classic paper that goes over basic asymptotics of bootstrap
\item Uses SLLN
\end{itemize}


\subsection{Chernick textbook}

\begin{itemize}
\item Great table comparing different CI methods with their assumptions (Ch 3, page 8)
\end{itemize}




\newpage

\section{Resolved questions}

\subsection{What should we do about logistic regression?}

Westfall's algorithm (pg 215) doesn't seem to work. He just regresses each outcome on the Xs and then generates from a binomial using the fitted parameters. But this doesn't preserve the correlation between the Ys (see \texttt{westfall\_logistic\_regression.R}), so I don't understand. Also, he says you can't do case-resampling for logistic regression, but I don't know why -- unless it's the same issue about correlated residuals that seems potentially problematic in OLS. 


\subsection{Should we compare our joint test to joint tests based on non-FWER procedures?}

If we reject joint null based on FWER control method, then we can conclude with 95\% confidence that this rejection is real, which implies that the joint null is false. 

With $k$-FWER control, we can conclude with 95\% confidence that there are fewer than $k$ false positives, so $R-k+1$ of the observed rejections are real (don't know which ones), where $R$ is the number of adjusted rejections. So, as long as $R>k-1$, we can reject the joint null with 95\% confidence. PROBLEM: WOULD HAVE TO TRY EVERY $k$??

With FDP control, we can reject the joint null if the proportion of adjusted rejections exceeds the allowed amount ($\gamma$). Again, problem because we'd have to try every $\gamma$?

FDR does not work for a joint test because it uses expected proportion of rejections. 

The problem is that $k$-FWER and FDP would probably lead to more powerful joint test than the FWER control method, but not clear which parameters to choose. So maybe we should not worry about them?


\subsection{What we can learn from existing FWER procedures?}

Say we observe 18 rejections at the 0.05 level. Then, with increasing stringency, we can say:
\begin{itemize}
\item Our null CI is $[0, 10]$. "We observed $18-10 = 8$ more hits than we would expect in 95\% of samples taken under the strong null (every null holds)."
\item $8-$FWER controlling procedure (Romano) rejects 11. "There are fewer than 8 false positives, so of these 14 rejections, at least $14-7 = 7$ are real effects, but we don't know which ones." (How should we decide which $k$ to use here?)
\item FWER-controlling procedure (Bonferroni or preferably Romano/Westfall/another resampling method) rejects 3. "All 3 of these rejections are real effects with 95\% confidence", or alternatively: "We reject the strong null with >95\% confidence."
\end{itemize}

In practice, you could get the first two from the same bootstrap resamples (taken parametrically under the strong null). Then you'd have to resample under the original distribution to get the third one. 


\subsection{Why do datasets created through full-case resampling reject the null too often?}

See my Cross Validated question (\url{https://stats.stackexchange.com/questions/323455/why-do-hypothesis-tests-on-resampled-datasets-reject-the-null-too-often}). There are two separate issues:

\begin{enumerate}
\item You have to center the bootstrap statistics by the estimated statistic in the original data (so do the hypothesis test on $\widehat{\beta}^{(j)} - \widehat{\beta}$, not on $\widehat{\beta}^{(j)}$ itself). This is because of the bootstrap analogy principle. 
\item With FCR, we violate the OLS assumption of iid errors because every repeated pair $(X,Y)$ has exactly the same residual, so the residuals are perfectly associated with $X$. If we use Westfall's \emph{null} resampling approach where we fix $X$ and resample residuals, then even though there are repeated observations, the \emph{residuals} of the model newly fit to the resampled data are still iid because we are breaking any association between the resampled residuals and $X$. This issue only matters if you need to meet parametric assumptions within each resample. Our application of bootstrapping is unusual in that we need to resample in a way that is compatible with parametric assumptions of the test we're going to apply for each outcome. This is why, e.g., Davison \& Hinkley do specifically recommend FCR for OLS. 
\end{enumerate}

It turns out that in the OLS example I tried, only issue #1 actually matter because fixing that issue led to nominal rejections. However, issue #2 seems to still be a real concern and might matter more in models less robust to assumption violations than OLS. 


\subsection{Why does Blakesley/Westfall’s minP bootstrapping work since it is an extreme order statistic?}
Probably because minP is not an order statistic of the data themselves. 

\subsection{If we go the confidence interval route, why not just treat the p-values as our data and resample directly from them to produce CI?}
Because there are only a fixed number of p-values (k), and this is not necessarily asymptotically large. 

\subsection{Why does bootstrapping work with small samples since we need the ECDF in sample to go to the true CDF? Or does it not work in small samples?}
A: Correct. It does not necessarily work in small samples. 



<!-- NEW SECTION -->
\newpage
\section{Simulation notes}

\subsection{Upcoming simulations}


\subsection{"2018-3-18 with other methods"}

We shelved the CI under the alternative because of the below issues (couldn't find a bootstrapping/resampling method that worked under $H_A$). In the present simulations, we compared our joint test of the null (using number of rejections) to other methods (Westfall's minP, Westfall's step-down, Bonferroni). 

\begin{itemize}
\item \textbf{Realized that regenerating residuals (method \texttt{h0.parametric}) actually doesn't work because it loses the correlated structure: we were just generating separate $Y$s. Empirically, saw that the null CIs were the same regardless of \texttt{rho.YY}.}
\item So these simulations used Westfall's reattach-residuals method (method \texttt{resid}), and all results made a lot of sense. 
\item CI plots shows the averaged CI limits and mean bootstrapped rejections by scenario. 
\item Seemed weird that both Westfall methods lead to exactly the same joint test, but this is okay. It's because the smallest p-values are the ones that change the least, so few p-values shift from being above to below 0.05 when changing from minP to step-down (see folder "Compare Westfall minP vs step-down"). 
\end{itemize}


\subsection{2018-1-23 unsaved local sims}

Revisiting residual resampling for CI: Fix design matrix, then re-generate residuals using $\widehat{\beta}$ and $\widehat{\sigma^2}$. (Maybe resampling the residuals would also work, but should we be worried about non-independence?)

\begin{itemize}
\item Tried a single outcome and different correlation strengths
\item Compared average rejection probability in bootstraps to average in original data
\item Need large $n$ for this! Using $n=1000$ didn't work because regression estimates aren't precise enough. Using $n=10,000$ seemed okay.
\item Not yet assessing CIs for number of rejections because would need multiple outcomes for that
\end{itemize}

Went back to debug FCR script and found that reattaching residuals works under null. 

Under alternative, though, got 25% rejections across all bootstrap resamples vs. 15% across 1000 sim reps in original datasets.

Still got 25% even when generating new residuals and adding them to the original fitted values!!!!!! Whyyyyyyyyyy???????????


\subsection{2018-1-22}

\begin{itemize}
\item Reduced number of outcomes to 30 to reduce computation time
\item Now also computing CIs based on raw percentiles (below) scaled by the estimated rate
\item And am trying with data generated under alternative
\item Still using uncorrelated outcomes
\item Rates are again almost perfectly estimated
\item \hl{We can't use subsampling when data are generated under $H_A$ because the number of rejections will be lower due to lower power. So the sampling distribution isn't just wider in the smaller samples; it doesn't even have the same expected value.}
\end{itemize}

\subsection{2018-1-20 round 2 subsampling}

\begin{itemize}
\item Running same as below, but with the real code instead of raw quantiles. "covers.correct" uses exactly rate = 0 as below; "covers" uses the rate estimate. 
\item Rates basically always estimated as 0 now because of larger $B$ 
\item Coverage now 80% instead of 70%
\item $\ast$Conclusion: $B=20,000$ instead of $B=2,000$ results in exactly correct rate estimation, at least for data generated under $H_0$. Can probably find middle ground of simulation reps.
\item Based on local simulations (debug\_subsampling), seems like a CI based just on percentiles of the subsamples is better than the one in the lab handout, where it's centered on the point estimate. 
\end{itemize}

One-sided CI idea to improve coverage a little

\subsection{2018-1-20 subsampling}

\begin{itemize}
\item Based on the exact variance I derived, when the null is true, the SE of $\widehat{\theta}$ doesn't depend on $n$ because tests maintain $\alpha$ probability of rejecting regardless of $n$. So in this case, the rate is exactly 0 (as correctly estimated by the last round of simulations).
\item So these simulations simplify things by not estimating the rate, but just taking the quantiles of the iterates for the third sample size ($n=900$). Seems like this should work fine. 
\item $\ast$Giving it the exact rate yields 100% coverage, up from 70% in previous simulation
\item Sample sizes same as below
\item Rep time 90th percentile: 1700 seconds with $B=2000$
\end{itemize}

\subsection{2018-1-19 - results not saved}

\begin{itemize}
\item Same as 2018-1-18, but increased all sample sizes by order of magnitude
\item Mean rate exactly 0 with min/max $pm 0.10$ and 25th, 75th percentiles $\pm 0.015$
\item Still 70% coverage
\item Used $n=100,000$ and subsample sizes $n_s = (400, 600, 900, 1350)$ to match lab handout, but I have $B = 2,000$ instead of $B = 20,000$
\end{itemize}

\subsection{2018-1-18 - results not saved}

\begin{itemize}
\item Trying subsampling with rate estimation in a null scenario and no correlation between outcomes
\item Only getting 70% coverage
\item Used $n=10,000$ and subsample sizes $n_s = (40, 60, 90, 135)$ to match lab handout, but I have $B = 2,000$ instead of $B = 20,000$
\item Each sbatch took ~3.5 hours to run (upper limit)
\end{itemize}

\subsection{2018-1-16 "under $H_0$ and FCR"}

\begin{itemize}
\item Joint test plot shows power of our joint test (i.e., resample residuals under $H_0$ and see if observed hits are extreme in this distribution) and compares to power of a naïve Bonferroni joint test (i.e., reject the joint null if any of the 100 Bonferroni-adjusted tests rejects).
\item Above results make sense: e.g., we are near the nominal joint alpha level (0.05 regardless of alpha level for individual tests) when data were generated under joint null. 
\item Excess hits plot shows average excess hits (hits above expectation) and CIs averaged across bootstraps. The CIs are sometimes completely above the observed excess hits. \textbf{This is because the FCR resampling approach is wrong.} See the resolved questions. 
\end{itemize}

\subsection{2017-8-1 (generate under $H_0$ and $H_A$)}

\begin{itemize}
\item FCR resampling doesn't seem to work because even when data generated under the null, the resamples have more than the expected number of rejections
\item Not due to lower truncation because using alpha = 0.5 doesn't help
\item Seems like correlations are preserved in individual resamples?
\item But I think the correlations are more variable, even if their mean is still 0, because sometimes we draw samples that have a lot of repetition
\end{itemize}

\subsection{Summary of previous two}

\begin{itemize}
\item{ Plot1 (CIs)
  \begin{itemize}
    \item Shows that the residual resampling is working correctly (since results are same regardless of whether we generate under H0 or under HA)
    \item As expected, more variable when Y’s are more correlated
  \end{itemize}
}
\end{itemize}


\subsection{2017-8-1 (generate under alternative)}

\begin{itemize}
\item Same as 2017-7-25, but now generating under weak alternative (rho.XY = 0.03) 
\item Expect same exact results for CIs (since they only use results from resampling under H0), but higher rejection rates
\end{itemize}


\subsection{2017-7-25 (generate under null)}

\begin{itemize}
\item CIs are 95\% two-sided ones for both values of alpha 
\item Hypothesis tests are one-sided and have sample alpha as the individual tests
\item N=1,000 and B=2,000 (j=10 per simulation)
\item Figure out why file 125 has extra rows (try running it again)

\item {\textbf{The apparently too-conservative behavior for the uncorrelated case is a benign artifact} of $\widehat{\theta}$'s discreteness.
  \begin{itemize}
    \item It occurs because when the Y’s are uncorrelated, the distribution of the number of rejections is less right-skewed, so it drops off more quickly in the tails. Since $\widehat{\theta}$ is discrete, we are forced to pick a quantile that doesn’t have exactly 5\% of the mass above it. By default, R inverts the ECDF, so uses the quantile with LESS than 5\% above it. Because the distribution of $\widehat{\theta}$ drops off quickly around the chosen quantile, we end up with conservative performance (see artifact\_plot1 and artifact\_plot2, where I shade in red the proportion that are above the variable’s own critical value).
    \item Artifact might improve a little with more simulation reps, but only to a point. 
    \item For this case, we could benchmark against the truth since $\widehat{\theta}$ is binomial. Indeed, the empirical quantiles of both \texttt{n.rej} and the bootstrap estimates are exactly correct. :) 
  \end{itemize}
}
\end{itemize}


\subsection{Summary so far}

\begin{itemize}
\item Seems like N=1,000 and B=2,000 is enough for good asymptotics 
\item When data are generated under null, resampling the Y’s or the residuals seems to works. But former theoretically should work only in more limited cases. 
\end{itemize}


\subsection{2017-7-24 “Sherlock sim”}

\begin{itemize}
\item Huge: N=10,000, B=10,000
\item Resampling under joint null (Y’s rather than residuals)
\item This seemed to work based on interim results (though I had trouble with existing stitching script, so stitched results in overall\_stitched folder might be wrong)
\end{itemize}


\subsection{2017-7-24 “Test smaller sim”}

\begin{itemize}
\item N=1,000, B=2,000 
\item Switched to resampling residuals instead of Ys themselves for the first time 
\item Seems to work! Going to try on cluster to have larger simulation
\end{itemize}


\subsection{2017-7-22}

\begin{itemize}
\item N=100, B=1,000: seems too small based on the below
\item Looked good at alpha=0.05 (4.4\% rejections) but maybe too conservative for alpha=0.01? (3.6\% rejections)
\item Took about 1.5 hours locally
\end{itemize}

\subsection{2017-7-21}

\begin{itemize}
\item With N=5,000, B=2,000, n.sims = 250, looks good
\item 5.5\% rejection rate on joint null for alpha = 0.05
\item 6\% rejection rate for alpha = 0.01
\item	Took about 30 hours locally
\item Seems good
\end{itemize}


\subsection{Summary so far}

\begin{itemize}
\item Resample under H0 by drawing Y’s separately; reject by inverted CI using its percentiles $\Rightarrow$ \textbf{WORKS} (see Sherlock sim)

\item Resample from original with single Y1; look at variance of mean $\Rightarrow$ \textbf{WORKS} 

\item{ Bootstrap from original sample with 100 independent std. normal; reject using z-test that’s only a function of mean because SD is treated as known $\Rightarrow$ \textbf{DOESN'T WORK} 
  \begin{itemize}
    \item	The bootstrapped distribution of the absolute differences is too variable compared to true distribution
    \item	Hence rejects only 1.8\% of time
  \end{itemize}
}


\item Conjecture: Hall \& Wilson approach seems to rely on the idea that the standardized estimator is pivotal, i.e., that it comes from a location-scale family. This is why we can get away with only sampling from the original distribution. Since our estimator definitely isn’t from a location-scale distribution, maybe that’s why it does not work. Also, based on the different results between currently running one and very first one, I think B=500 is not enough, and B = 10,000 is enough. Not sure about intermediate ones. 

\end{itemize}



\subsection{Summary so far}

\begin{itemize}
\item Seems like N=1,000 and B=2,000 is enough for good asymptotics 
\item When data are generated under null, resampling the Y’s or the residuals seems to works. But former theoretically should work only in more limited cases. 
\end{itemize}

For earlier simulation results, see old project log in Word. 

<!--
\newpage
Here is a test reference[@evalue]. Use semicolons to separate multiple references. To change reference style, Google for a .csl style sheet for journal of interest of one with similar reference style. 
-->


<!-- NEW SECTION -->
\section{Notes on Ying's MIDUS paper}

\subsection{Variables}
\begin{itemize}
\item Parental warmth ($X$): average of separate maternal and paternal warmth scales (so $\in [1,4]$)
\item {Flourishing (main $Y$)
  \begin{itemize}
  \item Continuous version: Sum of standardized subscales for emotional, psychological, and social well-being
  \item Binary version: Are you in top tertile on enough subscales?
  \item Count version: For how many subscales are you in top tertile? ($\in [0,13]$?)
  \end{itemize}
}
\item Subdomains comprising flourishing: 13 continuous measures (T3)
\item Bad outcomes (secondary $Y$s): 7 mental health problems and bad behaviors, all binary (T4)
\end{itemize}

\subsection{Analyses}
\begin{itemize}
\item Continuous flourishing and other continuous $Y$s: Normal GEE (cluster by siblings)
\item Binary flourishing: Poisson GEE with log link (risk ratios) (\hl{Did log-binomial not converge or other reason for choosing this?})
\item Count flourishing: Normal GEE (\hl{Why not, e.g., Poisson or NB?})
\item Bad binary outcomes: Poisson GEE for non-rare (RRs) or logistic regression for common (ORs)
\end{itemize}

\subsection{Our analyses}
\begin{itemize}
\item Multiple imputation for main analyses?
\item Randomly choose 1 sibling from each sibship to avoid clustering? Then just use OLS and regular log-linear model. Or could do mixed model, but then parametric bootstrap is more complicated.
\item Maybe treat outcomes as: 3 different flourishing variables + 7 binary bad outcomes? Also addresses model specification angle. 
\item 
\end{itemize}


\newpage
\section*{References}


