---
title: ''
author: ''
header-includes:
- \usepackage[T1]{fontenc}
- \usepackage{microtype}
- \usepackage[margin=1in]{geometry}
- \usepackage{fancyhdr}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead{}
- \fancyfoot{}
- \fancyhead[C]{Statistics for Multisite Replications}
- \fancyfoot[RO,LE]{\thepage}
- \usepackage{titlesec}
- \usepackage{booktabs}
- \usepackage{lettrine}
- \usepackage{paralist}
- \usepackage{setspace}\doublespacing
- \usepackage{natbib}
- \setcitestyle{apalike}
- \usepackage{url}
- \usepackage{parskip}
- \usepackage{color,soul}
- \usepackage{amsmath,amsfonts,amsthm, textcomp}
output:
  pdf_document:
bibliography: refs_mo.bib
---



<!---
---
title: ''
author: ''
header-includes:
- \usepackage[T1]{fontenc}
- \usepackage{microtype}
- \usepackage[margin=1in]{geometry}
- \usepackage{fancyhdr}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead{}
- \fancyfoot{}
- \fancyhead[C]{Multiple Testing}
- \fancyfoot[RO,LE]{\thepage}
- \usepackage{booktabs}
- \usepackage{lettrine}
- \usepackage{paralist}
- \usepackage{setspace}\singlespacing
- \usepackage{url}
- \usepackage{parskip}
- \usepackage{color,soul}
- \usepackage{palatino}
- \usepackage{amsmath,amsfonts,amsthm, textcomp}
- \usepackage{bm}
- \usepackage{caption}
- \usepackage{subcaption}
- \usepackage{tikz}
- \usepackage{verbatim}
- \usetikzlibrary{matrix}
- \usepackage{float}
- \usepackage{natbib}
- \setcitestyle{apalike}
output:
  pdf_document:
    citation_package: natbib
bibliography: refs_mo.bib
---
--->

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{fact}{Fact}
\newtheorem{example}{Example}
\newtheorem{assump}{Assumption}


\doublespacing

\begin{center}
\textbf{ \LARGE{Multiple Research Update \#2} } \\ \vspace{5mm}
\textbf{ \LARGE{\today} }
\vspace{10mm}
\end{center}

\doublespacing

\tableofcontents

<!--
\vspace{10mm}
\begin{center}
\large{ \emph{ Maya B. Mathur$^{1, 2\ast}$, Peng Ding$^{3}$, and Tyler J. VanderWeele$^{1,4}$ } }
\end{center}

\vspace{15mm}

\small{$^{1}$ Department of Biostatistics, Harvard T. H. Chan School of Public Health, Boston, MA, USA}

\small{$^{2}$Quantitative Sciences Unit, Stanford University, Palo Alto, CA, USA}

\small{$^{3}$Department of Statistics, University of California at Berkeley, Berkeley, CA, USA}

\small{$^{4}$Department of Epidemiology, Harvard T. H. Chan School of Public Health, Boston, MA, USA}


\vspace{15mm}
\begin{singlespacing} 
\small{$\ast$: Corresponding author:

mmathur@stanford.edu

Quantitative Sciences Unit (c/o Inna Sayfer)

1070 Arastradero Road

Palo Alto, CA

94305

}
\end{singlespacing}

\vspace{10mm}

-->

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}

\newpage

<!-------------------------- --------------------------------->
<!------------- SECTION: TO DO -------------->
<!-------------------------- --------------------------------->

\citet{westfalltext}

\section{To do}

\begin{itemize}
\item \textbf{Explain why residual resampling works creates samples generated under strong null}
\item \textbf{Think more broadly about proof and why it had to be so complicated}
\item \textbf{Check Remark 2 again}
\item \textbf{Fix references}
\item Connect the last theorem with number of rejections
\item Re-incorporate old material (e.g. general presentation)
\item Incorporate any of TVW's old comments
\end{itemize}

Say somewhere in paper:
\begin{itemize}
\item Unlike Freedman, we assume the resample size is same as original sample size ($N$)
\end{itemize}



<!-------------------------- --------------------------------->
<!------------- SECTION: INTRODUCTION -------------->
<!-------------------------- --------------------------------->

\section{Introduction}

Contributions of this paper:

\begin{itemize}
\item The new metrics
\item Theoretical justification for an existing resampling algorithm
\item Joint test power comparison of existing methods and its interesting results wrt the "na√Øve" methods (they're actually pretty good because differences emerge mainly in adjustment of the larger p-values)
\end{itemize}

<!-------------------------- --------------------------------->
<!------------- SECTION: SETTING AND NOTATION -------------->
<!-------------------------- --------------------------------->

\section{Setting and notation}

Suppose that $K$ random variables are measured on $N$ subjects, with the resulting matrix denoted $\mathbf{Z} \in \mathbb{R}_{N \times K}$. Let $Z_{nk}$ denote, for the $n^{th}$ subject, the $k^{th}$ random variable. Consider a resampling algorithm that generates, for iterate $j$, a dataset containing the random vector $\left( Z_{n1}^{(j)}, \dotsb, Z_{nK}^{(j)} \right)$ for each subject $n$. There are a total of $B$ resampled datasets. We use the superscript $^{(j)}$ to denote random variables, distributions, and statistics in resampled dataset $j$.

Further suppose that we conduct $W$ hypothesis tests, each with level $\alpha$. Denote the $w^{th}$ null hypothesis as $H_{0w}$. Let $c_{w,\alpha}$ be the critical value for the test statistic, $T_w$, of the $w^{th}$ test. We refer to the vector of test statistics as $\mathbf{T}$. <!--Assume that each test concerns a point null hypothesis (\textbf{Assumption 1}) and that each test is parametric with consistent estimates of the tested parameters in the likelihood identifiable from the observed data (\textbf{Assumption 2}).--> We define the "strong null" as the case in which all $W$ point null hypotheses hold and use the superscript $^0$ generally to denote distributions, data, or statistics generated under the strong null. 

Define the statistic corresponding to the total number of rejections in $W$ tests in a sample generated under the strong null as $\widehat{\theta}^0 = \sum_{w=1}^W 1 \big\{ T_w^0 > c_{w,\alpha} \big\}$. Similarly, the total number of rejections in resample $j$ is $\widehat{\theta}^{(j)} = \sum_{w=1}^W 1 \big\{ T_w^{(j)} > c_{w,\alpha} \big\}$. Using $F$ to denote cumulative distribution functions, respectively define the true CDF of the number of rejections under the strong null, its counterpart in the resamples, and its empirical estimator in the resamples as:
\begin{align*}
F_{\widehat{\theta}^0}(r) &= P \left( \widehat{\theta}^0 \le r \right) \\
F_{\widehat{\theta}^{(j)}}(r) &= P \left( \widehat{\theta}^{(j)} \le r \right) \\
\widehat{F}_{\widehat{\theta}^{(j)}} \left( r \right) &= \frac{1}{B} \sum_{j^{*}=1}^B 1 \Big\{ \widehat{\theta}^{(j^{*})} \le r \Big\}
\end{align*}



<!------------- --------------------------------->
<!-------------- SECTION: RESULTS --------------->
<!------------- --------------------------------->

\section{Main results}

We now show that under a certain class of resampling algorithms defined below, the empirical distribution of the number of rejections in the resamples converges to the true distribution of the number of rejections in samples generated under the strong null.


<!-------------- Definition 1 --------------->
<!-------------- \setcounter{assump}{2} --------------->
\bigskip \begin{assump}[Resampling under the strong null] 
\label{cbt_def}
The resampling algorithm used to generate $\left( Z_{n1}^{(j)}, \dotsb, Z_{nK}^{(j)} \right)$ must generate observations that are independently and identically distributed (i.i.d.), that fulfill the parametric assumptions of all hypothesis tests, and it must ensure that $\mathbf{T}^{(j)} \xrightarrow[B \to \infty]{D} \mathbf{T}^0$, i.e.:
$$\lim_{B \to \infty} F_{\mathbf{T}^{(j)}} = F_{\mathbf{T}^0}$$
\end{assump}

This requirement has several critical implications. First, the resampled data must preserve the correlation structure of all variables in the dataset, except where the strong null dictates otherwise. Otherwise, the distribution of the test statistics will typically not be preserved. Second, just as the original data are assumed to respect the parametric assumptions of all $W$ hypothesis tests, the resampled data must be generated in a manner that also respects this parametric structure. Otherwise, hypothesis tests conducted on the resampled data may not preserve their nominal $\alpha$-levels, which again affects the distribution of the test statistics. 


<!-------------- Example 1 ------------->
\bigskip
\begin{example}[A valid resampling algorithm for multiple linear regression] 
\label{ex:ols}
\normalfont
Suppose we conduct $W=2$ hypothesis tests using ordinary least squares (OLS) regressions of the first and second variables on the third variable, adjusting for the fourth:
\begin{align*}
Z_{n1} &= \beta_0 + \beta_1 Z_{n3} + \beta_2 Z_{n4} + \epsilon_{n1}\\
Z_{n2} &= \alpha_0 + \alpha_1 Z_{n3} + \alpha_2 Z_{n4} + \epsilon_{n2}
\end{align*}
with null hypotheses $H_{01}: \beta_1 = 0$ and $H_{02}: \alpha_1 = 0$, and assuming $\epsilon_{nw} \sim N(0, \sigma^2_w)$ for $w \in \{1,2\}$. A resampling algorithm \textcolor{red}{[@freedman]} satisifying Assumption \ref{cbt_def} could proceed by first fixing the covariates $Z_{n3}$ and $Z_{n4}$ for all observations while setting the "outcomes" equal to the fitted values plus a vector of residuals resampled with replacement. That is, letting $n'$ denote an observation sampled with replacement and letting hats denote the usual OLS estimates obtained from the original dataset, the resampled variables for observation $n$ are: 
\begin{align*}
Z_{n1}^{(j)} &:= \widehat{Z}_{n1} + \left( \widehat{Z}_{n'1} - Z_{n'1} \right)\\
Z_{n2}^{(j)} &:= \widehat{Z}_{n2} + \left( \widehat{Z}_{n'2} - Z_{n'2} \right)\\
Z_{n3}^{(j)} &:= Z_{n3}\\
Z_{n4}^{(j)} &:= Z_{n4}
\end{align*}
Then, as usual in bootstrap hypothesis testing [@hall], the test statistics are computed using $H_{01}: \beta_1 = \widehat{\beta}_1$ and $H_{02}: \alpha_1 = \widehat{\alpha}_1$ in order to recover the null distribution. This resampling algorithm fulfills Assumption \ref{cbt_def} (Appendix). Note that even if the original data were not generated under the strong null, this resampling algorithm guarantees that $E \big[ Z_{nw}^{(j)} \; \vert \; Z_{n3} \big] = E \big[ Z_{nw}^{(j)} \big]$ for $w \in \{1,2\}$, implying the strong null. This algorithm also retains the distributional assumption on $\epsilon_{n1}$ and $\epsilon_{n2}$.
\end{example}


<!-------------- Example 2  ------------->
\bigskip
\begin{example}[An invalid resampling approach for multiple linear regression]
\normalfont
We now return to the setting of Example \ref{ex:ols}. An alternative resampling approach that is intuitive, but in fact violates Assumption \ref{cbt_def}, is to again fix $Z_{n3}$ and $Z_{n4}$ but to resample with replacement the outcome vectors $\left( Z_{n'1}, Z_{n'2} \right)$ rather than the residuals:
\begin{align*}
Z_{n1}^{(j)} &:= {Z}_{n'1}\\
Z_{n2}^{(j)} &:= {Z}_{n'2}\\
Z_{n3}^{(j)} &:= Z_{n3}\\
Z_{n4}^{(j)} &:= Z_{n4}
\end{align*}
Although this approach indeed enforces the strong null and preserves the correlation between the outcomes, it fails to preserve the correlation between the outcomes and the adjusted covariate, $Z_{n4}$, and thus does not recover the distribution of $\left( \widehat{\beta}_1^0, \widehat{\alpha}_1^0 \right)$.
\end{example}

 

<!-------------- Example 2 ------------->
\bigskip
\begin{example}[Another invalid resampling algorithm for multiple linear regression] 
\normalfont
Another incorrect alternative would be to bootstrap parametrically while constraining $\beta_1 = \alpha_1 = 0$ to enforce the strong null:
\begin{align*}
Z_{n1}^{(j)} &:= \widehat{\beta}_0 + \widehat{\beta}_2 Z_{n4} + \epsilon_{n1}^{(j)}\\
Z_{n2}^{(j)} &:= \widehat{\alpha}_0 + \widehat{\alpha}_2 Z_{n4} + \epsilon_{n2}^{(j)}\\
Z_{n3}^{(j)} &:= Z_{n3}\\
Z_{n4}^{(j)} &:= Z_{n4}
\end{align*}
where $\epsilon_{nw}^{(j)} \sim_{i.i.d.} N(0, \widehat{\sigma}^2_{\epsilon_{nw}})$ for $w \in \{ 1,2\}$. However, this sequential algorithm fails to preserve the correlation between $Z_{n1}$ and $Z_{n2}$, in turn failing to recover the distribution of $\left( \widehat{\beta}_1^0, \widehat{\alpha}_1^0 \right)$.
\end{example}


<!-------------- Remark 1 --------------->
\bigskip
\begin{remark}
\label{CMT_results}
\normalfont
For Assumption \ref{cbt_def} to hold, it is sufficient for $\mathbf{T}$ to be a continuous function of $\mathbf{Z}$ and for $\mathbf{Z}^{(j)} \xrightarrow[B \to \infty]{D} \mathbf{Z}^0$, i.e., $\lim_{B \to \infty} F_{\mathbf{Z}^{(j)}} = F_{\mathbf{Z}^0}$. Note that this condition is \emph{not} necessary for Assumption \ref{cbt_def} to hold; for example, under the residual resampling scheme of Example \ref{ex:ols}, we have $E \big[ Z_{n1}^{(j)} \big] = E \big[Z_{n2}^{(j)} \big] = 0$ regardless of $E \big[ Z_{n1} \big]$ and $E \big[Z_{n2}\big]$.
\end{remark}



<!-------------- Lemma 1 --------------->
\bigskip \begin{lemma}
\label{lemma3}
The number of rejections in the resamples converges in distribution in $N$ to the number of rejections in samples from the original population under the strong null:
$$\lim_{B \to \infty} F_{{ \widehat{\theta} }^{(j)}} = F_{{ \widehat{\theta}^0 }}$$

\begin{proof}
Define the $r$-family of ``rejection sets'' as all possible configurations of the $W$ test statistics that lead to $r$ rejections: $$\mathcal{A}_r = \Big\{ \left( A_1, \dotsb, A_W \right) \in \mathbb{R}^W : \left( T_1 \in A_1, \dotsb, T_W \in A_W \right) \; \Rightarrow \; \widehat{\theta} = r\Big\}$$
(For example, $\mathcal{A}_5$ contains all the 5-vectors of sets such that, if $\left( T_1 \in A_1, \dotsb, T_5 \in A_5 \right)$, then there are 5 total rejections across the $W$ tests. Suppose, for example, that $T_w \in (-\infty, \infty) \; \forall \; w$ and that each null hypothesis is rejected when $T_w > c_{\alpha}$. Then there are ${W \choose r}$ vectors in $\mathcal{A}_5$, representing the number of ways to select $r$ rejections from $W$ tests. One such vector corresponds to rejecting the first 5 tests and failing to reject all remaining tests:\\ $\left( (c_\alpha, \infty), (c_\alpha, \infty), (c_\alpha, \infty), (c_\alpha, \infty), (c_\alpha, \infty), (-\infty, c_\alpha], \dotsb, (-\infty, c_\alpha] \right)$.)

\bigskip Now consider the distribution of $\widehat{\theta}^{(j)}$:
\begin{align*}
\lim_{B \to \infty} P \left( \widehat{\theta}^{(j)} = r \right) &= \lim_{B \to \infty} P \left( \sum_{w=1}^W 1 \Big\{ T_w^{(j)} > c_{w,\alpha} \Big\} = r \right) \\
&= \lim_{B \to \infty} \sum_{ \left( A_1, \dotsb, A_W \right) \in \mathcal{A} } P \left( T_1^{(j)} \in A_1, \dotsb, T_W^{(j)} \in A_W \right) \\
& \sum_{ \left( A_1, \dotsb, A_W \right) \in \mathcal{A} } P \left( T^0_1 \in A_1, \dotsb, T^0_W \in A_W \right) \tag{\text{from Assumption} \ref{cbt_def}}\\
&= P \left( \widehat{\theta}^0 = r \right)
\end{align*}
\end{proof}
\end{lemma}


<!-------------------------- --------------------------------->
<!-------------- SECTION: SIMULATION STUDY --------------->
<!-------------------------- --------------------------------->

\section{Simulation study}

\subsection{Methods}

We generated multivariate standard normal data, comprising 1 covariate ($X$) and 40 outcomes ($Y_1, \dotsb, Y_{40}$). The sample size was fixed at $N = 1000$. The correlation between each pair of outcomes was $\rho_{YY}$. The correlation between $X$ and a fixed proportion, $q$, of outcomes was $\rho_{XY}$ (with remaining pairs having correlation 0). 
In a full-factorial design, we manipulated:

\begin{itemize}
\item $\rho_{XY} \in \{ 0, 0.02, 0.03, 0.04, 0.05, 0.10 \}$
\item $\rho_{YY} \in \{ 0, 0.25, 0.50 \}$
\item $q \in \{ 0.50, 1 \}$
\item $\alpha \in \{ 0.01, 0.05 \}$
\end{itemize}

For each of $1000$ simulation reps per scenario, the simulation algorithm proceeded as follows:

\begin{enumerate}
\item Generate an observed dataset according to the scenario.
\item Compute $\widehat{\theta}$: For each outcome $Y_i$, regress $Y_i$ on $X$ and conduct a $t$-test at level $\alpha$ on the coefficient for $X$. 
\item{For each resampling iterate $j$ (with $B = 2000$), apply the algorithm in Example \ref{ex:ols}:
	\begin{enumerate}
	  \item Fix $X$ for all observations in the dataset.
    \item Draw observation index $n'$ randomly with replacement from $\{ 1, \dotsb, N \}$. For observation $n$, set $Y_{nw}^{(j)} = \widehat{Y}_{n'w} - Y_{n'w}$ (the regression residual for observation $n'$ in the original dataset).
    \item Compute $\widehat{\theta}^{(j)}$: Regress $Y_{w}^{(j)}$ on $X$ and conduct a $t$-test at level $\alpha$ on the coefficient for $X$. 
    \end{enumerate}
}
\item Use the quantiles of $\left( \widehat{\theta}^{(1)}, \dotsb, \widehat{\theta}^{(B)} \right)$ to construct a 95\% CI for $\theta_0$ and to check if $\widehat{\theta}$ is above the $1-\alpha$ quantile.
\end{enumerate}


\subsection{Power to test the strong null}

Figure 2 shows the power of joint tests of the strong null constructed using existing familywise error rate (FWER) procedures, for which we defined rejection of the strong null as rejecting at least one of the 40 FWER-corrected hypothesis tests. We compared two non-resampling methods (Bonferroni and Holm), four resampling methods that use data resampled under the strong null (Westfall minP[@westfalltext], Westfall step-down[@westfalltext], our method with $\alpha=0.05$, and our method with $\alpha=0.01$), one resampling method using data resampled without constraints (Romano[@romano]), and our highly experimental procedure involving rejection based on the mean log $p$-value. 

For resampling methods using data generated under the null, we used the algorithm in Example \ref{ex:ols}. For the single method using data generated without constraints (Romano), we used a classical nonparametric bootstrap in which we simply resampled entire rows of the dataset with replacement and conducted hypothesis tests of the bootstrapped estimates versus those in the original dataset (rather than 0) to recover the null distribution [@hall;@chernick]. However, note that the classical bootstrap does not guarantee correct inference in the fitted OLS models because the exact repetition of entire rows of data induces correlation between $X$ and the residuals; we chose this approach because we are unaware of an unconstrained resampling algorithm for OLS that preserves parametric assumptions. (In contrast, residual resampling as in Example \ref{ex:ols} circumvents this problem because entire rows are not repeated.) Nevertheless, the joint test based on Romano's method under the classical bootstrap showed nominal rejection rates in the null scenarios (Figure 2, upper left panel).


![Power of joint tests based on various FWER control procedures versus our methods.](joint_test.png)

\subsection{Observed rejections vs. 95\% confidence interval under strong null}

Figure 3 shows these results. (As we discussed, for the next iteration of this writeup, I'll remove the scenarios with small effect sizes; they are just included here for completeness.)

![Average number of rejections and confidence limits in datasets resampled under the null when the original data were generated under various null and alternative scenarios.](null_ci.png)




<!-------------------------- --------------------------------->
<!-------------- SECTION: APPLIED EXAMPLE --------------->
<!-------------------------- --------------------------------->

\section{Applied example}




<!-------------------------- --------------------------------->
<!-------------- SECTION: APPENDIX --------------->
<!-------------------------- --------------------------------->

\newpage

\begin{center}
\textbf{ \LARGE{Appendix} } \\ \vspace{5mm}
\vspace{10mm}
\end{center}


\section{Validity of residual resampling for OLS regression}

\subsection{Regression setting and notation}


Here we modify notation in the main text to accommodate the OLS setting. In general, we will usually use superscripts to denote lengths and subscripts to denote indices. $[\cdot]_{ij}$ denotes the $(i,j)^{th}$ element of a matrix, and $[\cdot]_{i}$ denotes the $i^{th}$ element of a vector. Suppose that each of $W$ outcome variables is regressed on the same $N \times p$ design matrix, $X$. Assume that $X$ contains an intercept term, such that the residuals have mean 0, and that all other covariates are mean-centered. Let $\epsilon_w = \left( \epsilon_{1w}, \cdots, \epsilon_{Nw} \right)$ denote the $N$-vector of true errors for the $w^{th}$ regression and $\widehat{\epsilon}_w$ its estimated counterpart (the residuals). Let $\sigma^2_w = E \big[ \epsilon_{nw}^2 | X \big]$ as usual. Let $\beta^W = \left( \beta_1, \dotsb, \beta_W \right)$ be a vector containing, for each of $W$ regression models, the single coefficient of interest, and let $\widehat{\beta}^W$ denote its sample estimate. (We use the superscript to remind the reader that this is not the usual $p$-vector of coefficients from a single multiple regression model.) The $W$ test statistics of interest are the usual $Z$-statistics $\left( \frac{\widehat{\beta}_1}{  \widehat{\sigma}^2_1 \left( X'X\right)^{-1} }, \dotsb, \frac{\widehat{\beta}_W}{  \widehat{\sigma}^2_W \left( X'X\right)^{-1} } \right)$. 

Additionally, in order to control the 

assume the following regularity condition on the design matrix, which allows :

bookmark


<!-------------------------- REGULARITY ASSUMPTION ---------------------------->

\begin{assump}
\label{regularity_assump}
Let $i \in \{ 2, \dotsb, p\}$ be the index of the regression covariate of interest and $B \in \mathbb{R}^{N\times1}$ be the transposed $i^{th}$ row of $(X'X)^{-1}X'$, or equivalently the $i^{th}$ column of $X (X'X)^{-1}$. Assume that for some constant $k>0$:
\begin{align*}
N \cdot B'B &\xrightarrow[N \to \infty]{P} k \\
\Leftrightarrow N \sum_{n=1}^N [(X'X)^{-1}X']^2_{in} &\xrightarrow[N \to \infty]{P} k
\end{align*}
\end{assump}

\begin{remark}
To provide intuition, consider the assumption in the context of simple linear regressions ($p=2$), with $X_2$ denoting the exposure of interest. Define $S_1 = \sum_{n=1}^N X_{n2}$ and $S_2 = \sum_{n=1}^N X_{n2}^2$. Then, for each of $W$ such models, we have:
\begin{align*}
X'X &= \begin{bmatrix}
N & S_1 \\
S_1 & S_2 
\end{bmatrix}\\ \\
(X'X)^{-1} &= \frac{1}{N S_2 - S_1^2} \begin{bmatrix}
S_2 & -S_1 \\
-S_1 & N 
\end{bmatrix}\\ \\
(X'X)^{-1}X' &= \frac{1}{N S_2 - S_1^2} \begin{bmatrix}
S_2 - X_{11} S_1 & \dotsb & S_2 - X_{N1} S_1 \\
-S_1 + X_{11}N & \dotsb & -S_1 + X_{N1} N 
\end{bmatrix}\\
\end{align*}

The regularity condition concerns the term:
\begin{align*}
N \sum_{n=1}^N B_n^2 &= N \sum_{n=1}^N [(X'X)^{-1}X']^2_{2n} \\
&= \frac{1}{ N S_2^2 - 2 S_2 S_1^2 + \frac{1}{N} S_2} \sum_{n=1}^N \left( X_{n2} N - S_1 \right)^2 \\
&= \frac{1}{ N S_2^2 - 2 S_2 S_1^2 + \frac{1}{N} S_2} \sum_{n=1}^N \left( N^2 X_{n2}^2 - 2N X_{n2} S_1 + S_1^2 \right) \\
&= \frac{N^2 S_2 - 2N S_1^2 + NS_1^2 }{N S_2^2 - 2 S_2 S_1^2 + \frac{1}{N} S_2} \\
&= \frac{N^2 S_2 - NS_1^2 }{N S_2^2 - 2 S_2 S_1^2 + \frac{1}{N} S_2} \\
&= \frac{\frac{S_2}{N} - \frac{S_1^2}{N^2} }{ \frac{S_2^2}{N^2} - 2 \frac{S_2}{N} \frac{S_1^2}{N^2} + \frac{S_2}{N^2} } \tag{$\div N^3$} \\
&\xrightarrow[N \to \infty]{P} 1 / \sigma^2_w
\end{align*}
The last line follows because, by mean-centering, we have $S_1/N \xrightarrow[N \to \infty]{P} 0$ and $S_2/N \xrightarrow[N \to \infty]{P} \sigma^2_w$. Thus, for simple linear regressions, Assumption \ref{regularity_assump} holds if $\sigma^2_w > 0$ for all $w \in \{ 1, \cdots, W\}$. 
\end{remark}

We will consider validity of the bootstrap in terms of convergence on the Mallows-Wasserstein metric, a conventional choice that is defined as follows [@dasgupta; @freedman].

\begin{definition}
\label{mallows}
Let $G_A$ and $G_B$ be arbitrary marginal distribution functions for random vectors $A \in \mathbb{R}^W$ and $B \in \mathbb{R}^W$, respectively. Then the Mallows-Wasserstein distance between $G_A$ and $G_B$ is the infimum, taken over all possible joint distributions for $(A,B)$ such that $A \sim G_A$ and $B \sim G_B$ marginally, of the expected squared Euclidean norm between $A$ and $B$:

\begin{align*}
d_2^W \left( G_A, G_B \right) & := \inf\limits_{ \substack{A \sim G_A \\ B \sim G_B} } E \big[ || A - B ||^2 \big]
\end{align*}
\end{definition}

We proceed to prove that the residual-resampling bootstrap is consistent with respect to the Mallows-Wasserstein metric in a development that closely follows @freedman, who considered the asymptotic validity of residual resampling in recovering the sampling distribution of a $p$-vector of coefficient estimates from a single multiple linear regression model. Here, we extend this work to consider the sampling distribution of $\widehat{\beta}^W$. We first establish a series of lemmas paralleling those of @freedman and @bickel.


<!-------------------------- LEMMA 8.9' ---------------------------->

\begin{lemma}[Lemma 8.9']
\label{trace_bound}
Let $C$ and $D$ be $W \times N$ random matrices, and let $B$ be a fixed $N$-vector. Let $G_C$ and $G_D$ be the marginal distribution functions of the $W$-vectors $CB$ and $DB$, respectively. Then: 
\begin{align*}
d_2^W \left( G_C, G_D \right)^2 &\le \text{tr} \{ BB' \cdot E \big[ (C-D)'(C-D) \big] \}
\end{align*}

\begin{proof}
\begin{align*}
d_2^W \left( G_C, G_D \right)^2 &\le E \Big[ || CB - DB ||^2 \Big]\\
&= E \Big[ \text{tr} \{ \underbrace{(CB-DB)}_{W \times 1} \underbrace{(CB-DB)'}_{1 \times W} \} \Big] \\
&= E \Big[ \text{tr} \{ \underbrace{(C-D)}_{W \times N} \underbrace{BB'}_{N \times N} \underbrace{(C-D)'}_{N \times W} \} \Big] \\
&= E \Big[ \text{tr} \{ \underbrace{ BB' (C-D)'(C-D) }_{N \times N} \} \Big] \tag{trace cyclical invariance}\\
&= \text{tr} \{ E \Big[ BB' (C-D)'(C-D) \Big] \}  \\
&= \text{tr} \{ BB' \cdot E [ \underbrace{ (C-D)'(C-D) }_{N \times N} ] \} \tag{$B$ is fixed} \\
\end{align*}
\textcolor{red}{In the inequality, the LHS is the infimum of the expectation over all possible pairs of random variables with the marginal distributions $G_C$ and $G_D$, whereas the RHS is the expectation for a particular pair of random variables (see similar step in Bickel \& Freedman pg 1214, Lemma 8.9).}
\end{proof}
\end{lemma}




<!-------------------------- THEOREM 2.1'---------------------------->
The next theorem bounds the distance between the true sampling distribution of the test statistics and the estimated sampling distribution in the resamples in terms of the distance between the sampling distribution of the true errors and the resampled residuals. 

\begin{theorem}[Theorem 2.1']
\label{bound_thm}
Paralleling @freedman's notation, let $F$ denote the true distribution function of the true errors, $\left( \epsilon_{n1}, \cdots, \epsilon_{nW} \right)$, and let $\widehat{F}_N$ denote the empirical distribution function of the residuals, which is used to approximate $F$ in residual resampling. Let $\Psi(F)$ denote the distribution of the standardized coefficient estimates, $\sqrt{N} \left( \widehat{\beta}^W - \beta^W \right)$ that are constructed as a function of the true error distribution; $\Psi(F)$ therefore represents the true sampling distribution to which a valid bootstrapped sampling distribution must converge. In contrast, let $\Psi(\widehat{F}_N)$ the distribution of the standardized coefficient estimates in which the empirical distribution of the residuals is substituted for the true distribution, i.e. $\sqrt{N} \left( \widehat{\beta}^{W(j)} - \widehat{\beta}^W \right)$\footnote{Note that the resampled test statistics are BOOKMARK, }.


As in Assumption \label{regularity_assump}, let $B \in \mathbb{R}^{N\times1}$ be the $i^{th}$ column of $(X'X)^{-1}X'$. Then:
\begin{align*}
d_2^W \left( \Psi(F), \Psi(\widehat{F}_N) \right)^2 &\le N \cdot \text{tr} \{ BB' \} \cdot d_2^W \left( F, \widehat{F}_N \right)^2
\end{align*}
\begin{proof}
In general for multiple regression, we have $\widehat{\beta} - \beta = \left( X'X\right)^{-1} X' \epsilon$.
Letting:
\begin{align*}
C \in \mathbb{R}^{W \times N} =
\begin{bmatrix}
    \text{---} \hspace{-0.2cm} & \epsilon_1' & \hspace{-0.2cm} \text{---} \\
    & \vdots & \\
    \text{---} \hspace{-0.2cm} & \epsilon_W' & \hspace{-0.2cm} \text{---}
\end{bmatrix}
\end{align*}
we can express $\Psi(F)$ as the distribution of the $W$-vector: 
\begin{align*}
\sqrt{N} \left( \widehat{\beta}^W - \beta^W \right) &= \sqrt{N} \begin{bmatrix}
    [ (X'X)^{-1}X' \epsilon_1 ]_{i}\\
    \vdots\\
    [ (X'X)^{-1}X' \epsilon_W ]_{i}
\end{bmatrix} = \sqrt{N} \begin{bmatrix}
    \epsilon_1' B \\
    \vdots\\
    \epsilon_W' B
\end{bmatrix} =
\sqrt{N} \cdot CB
\end{align*}
whose $w^{th}$ element pertains to the $i^{th}$ regression coefficient in the $w^{th}$ regression. Now let $D$ be the counterpart of $C$ with the residual vectors, $\widehat{\epsilon}_w$, in place of the true errors, $\epsilon_w$. Then the entries of matrix $(C-D)'(C-D)$ are:
\begin{align*}
[(C-D)'(C-D)]_{kj} &= \sum_{w=1}^W [(C-D)']_{kw} [C-D]_{wj} \\
&= \sum_{w=1}^W [C-D]_{wk} [C-D]_{wj} \\
&= \sum_{w=1}^W \left( \epsilon_{jw} - \widehat{\epsilon}_{jw} \right) \left( \epsilon_{kw} - \widehat{\epsilon}_{kw} \right)
\end{align*}
We have $E[ \left( \epsilon_{jw} - \widehat{\epsilon}_{jw} \right) \left( \epsilon_{kw} - \widehat{\epsilon}_{kw} \right) ] = \text{Cov} \left( \epsilon_{jw} - \widehat{\epsilon}_{jw}, \epsilon_{kw} - \widehat{\epsilon}_{kw} \right)$, but for all $k \ne j$, the covariance is 0 because the observations are independent. Thus, letting $I^{N}$ denote the $N \times N$ identity matrix, we have that $E[(C-D)'(C-D)]$ is a diagonal matrix equal to $I^{N} \cdot E \Big[ \sum_{w=1}^W \left( \epsilon_{jw} - \widehat{\epsilon}_{jw} \right)^2 \Big] = I^{N} \cdot d_2^W \left( \widehat{F}_N, F\right)$. \textcolor{red}{I took the latter equality from B\&F Lemma 8.9 but don't understand it. I traced it back to the proof of Lemma 8.6 (pg 1213), in which this equality is stated to hold "without loss of generality in view of Lemma 8.1", but I still don't understand.} The result then follows immediately from applying Lemma \ref{trace_bound}, setting $G_C = F$, $G_D = \widehat{F}_N$ and $B, C,$ and $D$ as defined above and pulling the scalar $\sqrt{N}$ outside the squared distance. 
\end{proof}
\end{theorem}

To apply the bound in Theorem \ref{bound_thm}, we will first bound the term on the right-hand side using a triangle inequality, which applies because $d_2^W(\cdot, \cdot)$ is a metric [@bickel]. To this end,  let $F_N$ denote the empirical distribution function of the true error vector, $\epsilon^W$. Then we have the following triangle inequality:
\begin{align}
\label{triangle}
d_2^W \left( \widehat{F}_N, F \right)^2 \le d_2^W \left( \widehat{F}_N, F_N \right)^2 + d_2^W \left( F_N, F \right)^2 
\end{align}
The first term on the RHS relates the empirical distribution of the residuals to the empirical distribution of the true errors (which are discrete distributions taking $N$ values); the second term relates the latter empirical distribution to the true error distribution (which is continuous by standard OLS assumptions). 

\begin{lemma}[Lemma 2.1']
\label{first_bound}
Concerning the first term on the RHS of Equation (\ref{triangle}):
\begin{align*}
d_2^W \left( \widehat{F}_N, F_N\right)^2 \xrightarrow[N \to \infty]{P} 0
\end{align*}


\begin{proof}
As in Definition \ref{mallows}, let $U \sim \widehat{F}_N$ and $V \sim F_N$ be arbitrary random variables in $\mathbb{R}^W$ that follow the empirical marginal distributions of the residuals and true errors. Denote their elements $\left( U_1, \dotsb, U_W \right)$ and $\left( V_1, \dotsb, V_W \right)$. \textcolor{red}{Let $(\widetilde{U}, \widetilde{V})$ be a special choice of $(U,V)$ that follow not only the marginal empirical distributions $\widehat{F}_N$ and $F_N$, but also the empirical \emph{joint} distribution of the residuals and the true errors.}


\begin{align*}
d_2^W \left( \widehat{F}_N, F_N\right)^2 & := \inf\limits_{ \substack{U \sim \widehat{F}_N \\ V \sim F_N} } E \big[ || U - V ||^2 \big] \\
& \textcolor{red}{\le E \Big[ || \widetilde{U} - \widetilde{V} ||^2 \Big] \tag{\text{choose one element from inf set}} }\\
& = \frac{1}{N} \sum_{n=1}^N \underbrace{ \sum_{w=1}^W \left( \widehat{\epsilon}_{nw} - \epsilon_{nw} \right)^2 }_{||\cdot||^2 \text{ of a $W$-vector}} \tag{\text{joint ECDF expectation}}\\
& = \frac{1}{N} \sum_{w=1}^W \underbrace{ \sum_{n=1}^N \left( \widehat{\epsilon}_{nw} - \epsilon_{nw} \right)^2 }_{||\cdot||^2 \text{ of an $N$-vector}}\\
&= \frac{1}{N} \sum_{w=1}^W || \widehat{\epsilon}_w - \epsilon_w ||^2 \\
\Rightarrow E \Big[ d_2^W \left( \widehat{F}_N, F_N \right)^2 \Big] &= \frac{p}{N} \sum_{w=1}^W \sigma^2_w \tag{\text{Freedman Eq. (2.2)}} \\
& \xrightarrow[N \to \infty]{} 0
\end{align*}
The interchange of summations in lines 3-4 is used to express $W$ norms involving residuals from different regressions, summed over $N$ observations, as $N$ norms involving residuals of observations within a regression, summed over $W$ regressions. The latter is more convenient because it allows application of existing theory for a single multiple regression model.  


\end{proof}
\end{lemma}

\begin{lemma}
\label{second_bound}
Concerning the expectation of the second term on the RHS of Equation (\ref{triangle}):
\begin{align*}
E \Big[ d_2^W \left( F_N, F \right)^2 \Big] \xrightarrow[N \to \infty]{P} 0
\end{align*}

\begin{proof}
Letting $P_N$ denote an empirical probability, $F_N$ can be expressed as:
\begin{align*}
P_N \left( \epsilon_{n1} \le c_1, \dotsb, \epsilon_{nW} \le c_W \right) &= \frac{1}{N} \sum_{n=1}^N 1 \big\{ \epsilon_{n1} \le c_1, \dotsb, \epsilon_{nW} \le c_W \big\} \\
&\xrightarrow[N \to \infty]{P} P \left( \epsilon_{n1} \le c_1, \dotsb, \epsilon_{nW} \le c_W \right) \tag{SLLN}
\end{align*}
which is the joint cumulative distribution corresponding to $F$. By @bickel's Lemma 8.3, this convergence in probability implies the desired result.
\end{proof}
\end{lemma}


\begin{theorem}
The residual bootstrap is weakly consistent under the Mallows-Wasserstein metric for the OLS coefficient estimates [@dasgupta]; that is:
\begin{align*}
d_2^W \left( \Psi(F), \Psi(\widehat{F}_N) \right)^2 \xrightarrow[N \to \infty]{P} 0
\end{align*}

\begin{proof}

Combining Theorem \ref{bound_thm} with the triangle inequality in Equation (\ref{triangle}) and observing that $\text{tr} \{ BB' \} = \sum_{n=1}^N B_N^2 \ge 0$ yields:
\begin{align*}
d_2^W \left( \Psi(F), \Psi(\widehat{F}_N) \right)^2 &\le N \cdot \text{tr} \{ BB' \} \cdot \left( d_2^W \left( \widehat{F}_N, F_N \right)^2 + d_2^W \left( \widehat{F}_N, F \right)^2 \right)
\end{align*}

The term $N \cdot \text{tr} \{ BB' \} \xrightarrow[N \to \infty]{P} k$ by Assumption \ref{regularity_assump}. By Lemma \ref{first_bound}, $d_2^W \left( \widehat{F}_N, F \right)^2 \xrightarrow[N \to \infty]{P} 0$. Last, the convergence in $r^{th}$ mean of Lemma \ref{second_bound} implies that $d_2^W \left( F_N, F \right)^2 \xrightarrow[N \to \infty]{P} 0$, so the desired result holds. 


Big picture (bookmark)

This shows that test stats generated as a function of the ECDF of the residuals ($\widehat{F}_N$) converge to test stats generated from the true error distribution ($F$). The bootstrap residual distribution can be made arbitrarily close to $\widehat{F}_N$ by taking $B \to \infty$, so by convention [@freedman], we ignore this source of error. 

Null issue



\section{In progress: connection to number of rejections}
The estimated standard errors are $\left( (X'X)^{-1}\widehat{\sigma}^2_1, \dotsb, (X'X)^{-1}\widehat{\sigma}^2_W \right)$, which converge to a point mass at $\left( (X'X)^{-1}\sigma^2_1, \dotsb, (X'X)^{-1}\sigma^2_W \right)$, so the desired convergence result holds for the joint distribution of the test statistics $\frac{ \widehat{\beta}_w }{ (X'X)^{-1} \widehat{\sigma}^2_w }$ (see @freedman's Theorem 2.2)
\end{proof}
\end{theorem}


* Getting to number of rejections: 





\newpage
\section*{References}

\bibliography{refs.bib}
\bibliographystyle{apacite}

